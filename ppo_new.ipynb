{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yfinance as yf\n",
    "from vnstock import Vnstock\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "from gymnasium import spaces\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn, optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import talib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "from datetime import datetime\n",
    "\n",
    "from stable_baselines3 import SAC, PPO, A2C\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_stock_data(vn_stock_list, global_stock_list, starting, ending) -> pd.DataFrame:\n",
    "    date_range = pd.date_range(start=starting, end=ending, freq='D')\n",
    "    stock_data_dict = {}\n",
    "    \n",
    "    print(f\"Fetching {len(vn_stock_list)} Vietnamese stock data...\")\n",
    "    for stock in vn_stock_list:\n",
    "        try:\n",
    "            data = Vnstock().stock(symbol=stock, source='VCI').quote.history(\n",
    "                start=starting, end=ending\n",
    "            )\n",
    "            # Reset index and rename 'time' to 'date'\n",
    "            data.reset_index(inplace=True)\n",
    "            data.rename(columns={'time': 'date'}, inplace=True)\n",
    "            # Convert 'date' to datetime and set as index\n",
    "            data['date'] = pd.to_datetime(data['date'])\n",
    "            data.set_index('date', inplace=True)\n",
    "            # Select relevant columns\n",
    "            data = data[['open', 'high', 'low', 'close']]\n",
    "            # Reindex to full date range\n",
    "            data = data.reindex(date_range, fill_value=pd.NA)\n",
    "            # Create multi-index with stock symbol and metrics\n",
    "            data.columns = pd.MultiIndex.from_product([[stock], data.columns], names=['Stock', 'Metric'])\n",
    "            stock_data_dict[stock] = data\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {stock}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Fetching {len(global_stock_list)} Global stock data...\")\n",
    "    for stock in global_stock_list:\n",
    "        try:\n",
    "            data = yf.download(stock, start=starting, end=ending)\n",
    "            # Ensure index is datetime\n",
    "            data.index = pd.to_datetime(data.index)\n",
    "            # Reindex to full date range\n",
    "            data = data.reindex(date_range, fill_value=pd.NA)\n",
    "            # Select and rename columns to match Vietnamese stock format\n",
    "            data = data[['Open', 'High', 'Low', 'Close']]\n",
    "            data.columns = ['open', 'high', 'low', 'close']  # Lowercase for consistency\n",
    "            \n",
    "            price_columns = ['open', 'high', 'low', 'close']\n",
    "            data[price_columns] = data[price_columns] * 25.5  # Multiply by 25.5 for USD to VND conversion\n",
    "            \n",
    "            # Limit to 2 decimal places\n",
    "            data[price_columns] = data[price_columns].round(2)\n",
    "            \n",
    "            # Create multi-index with stock symbol and metrics\n",
    "            data.columns = pd.MultiIndex.from_product([[stock], data.columns], names=['Stock', 'Metric'])\n",
    "            stock_data_dict[stock] = data\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {stock}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Concatenate all stock data horizontally\n",
    "    if stock_data_dict:\n",
    "        combined_data = pd.concat(stock_data_dict.values(), axis=1)\n",
    "        \n",
    "        # Fill with both forward and backward\n",
    "        combined_data.ffill(inplace=True)\n",
    "        combined_data.bfill(inplace=True)\n",
    "        \n",
    "        # Set title for all columns\n",
    "        combined_data.columns = pd.MultiIndex.from_tuples(combined_data.columns, names=['Stock', 'Metric'])\n",
    "        \n",
    "        # Sort columns by stock name\n",
    "        combined_data = combined_data.sort_index(axis=1, level='Stock')\n",
    "        \n",
    "        return combined_data\n",
    "    else:\n",
    "        raise ValueError(\"No stock data was successfully fetched.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoggingCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0, log_interval=500, save_path=\"plots\", filename=\"training_rewards.pdf\"):\n",
    "        super(CustomLoggingCallback, self).__init__(verbose)\n",
    "        self.rewards = []\n",
    "        self.steps = []\n",
    "        self.log_interval = log_interval\n",
    "        self.save_path = save_path\n",
    "        self.filename = filename\n",
    "\n",
    "        # Ensure the save directory exists\n",
    "        os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        reward = self.locals['rewards'][0]\n",
    "        step = self.num_timesteps\n",
    "\n",
    "        self.rewards.append(reward)\n",
    "        self.steps.append(step)\n",
    "\n",
    "        if self.verbose > 0 and step % self.log_interval == 0:\n",
    "            avg_reward = np.mean(self.rewards[-self.log_interval:])\n",
    "            max_reward = np.max(self.rewards[-self.log_interval:])\n",
    "            min_reward = np.min(self.rewards[-self.log_interval:])\n",
    "            print(f\"Step: {step}, Avg Reward: {avg_reward}, Max Reward: {max_reward}, Min Reward: {min_reward}\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        if self.verbose > 0:\n",
    "            total_rewards = sum(self.rewards)\n",
    "            print(f\"Training finished. Total rewards: {total_rewards}\")\n",
    "            print(f\"Total steps: {self.steps[-1]}\")\n",
    "            self.plot_training(self.rewards)\n",
    "\n",
    "    def plot_training(self, rewards):\n",
    "        sma = np.convolve(rewards, np.ones(50) / 50, mode='valid')\n",
    "\n",
    "        plt.figure()\n",
    "        plt.title(\"Training Rewards\", fontsize=14, fontweight='bold')\n",
    "        plt.plot(rewards, label='Raw Reward', color='#F6CE3B', alpha=1)\n",
    "        plt.plot(sma, label='SMA 50', color='#385DAA')\n",
    "        plt.xlabel(\"Step\", fontsize=12, fontweight='bold')\n",
    "        plt.ylabel(\"Rewards\", fontsize=12, fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Save the plot as PDF\n",
    "        filepath = os.path.join(self.save_path, self.filename)\n",
    "        plt.savefig(filepath, format='pdf')\n",
    "\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnv(gym.Env):\n",
    "    def __init__(\n",
    "        self, \n",
    "        df: pd.DataFrame,\n",
    "        window_size: int, \n",
    "        frame_bound: tuple,\n",
    "        initial_balance: int, \n",
    "        max_shares: int,      \n",
    "        trade_max: int = 100,   \n",
    "        trade_fee_percent: float = 0.001,\n",
    "        risk_free_rate: float = 0.01 / 252  # Annual risk-free rate converted to daily\n",
    "    ):\n",
    "        self.window_size = window_size\n",
    "        self.frame_bound = frame_bound\n",
    "        self.initial_balance = initial_balance\n",
    "        self.max_shares = max_shares\n",
    "        self.trade_max = trade_max\n",
    "        self.trade_fee_percent = trade_fee_percent\n",
    "        self.risk_free_rate = risk_free_rate\n",
    "        \n",
    "        self.df = self.add_technical_indicators(df)\n",
    "        \n",
    "        self.prices, self.signal_features = self._process_data()\n",
    "        self.shape = (self.window_size, self.signal_features.shape[1])\n",
    "        super().__init__()\n",
    "\n",
    "        # Define action space - one action per stock\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(len(df.columns.levels[0]),), dtype=np.float64)\n",
    "        \n",
    "        # Define observation space\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=self.shape,\n",
    "            dtype=np.float64\n",
    "        )\n",
    "        \n",
    "        # Episode state variables\n",
    "        self._start_tick = self.window_size\n",
    "        self._end_tick = len(self.prices) - 1\n",
    "        self._truncated = None\n",
    "        self._current_tick = None\n",
    "        self._last_trade_tick = None\n",
    "        self._total_reward = None\n",
    "        self._total_profit = None\n",
    "        self._trade_count = None\n",
    "        self.history = None\n",
    "        \n",
    "        # Trading statistics variables\n",
    "        self._balance = None\n",
    "        self._shares_held = None\n",
    "        self._portfolio_value = None\n",
    "        self._prev_portfolio_value = None\n",
    "        self._last_action = None\n",
    "        self._peak_portfolio_value = None\n",
    "        self._transaction_cost = 0\n",
    "        self._step_transaction_cost = 0 \n",
    "\n",
    "        # Reward tracking variables\n",
    "        self.min_reward = None\n",
    "        self.max_reward = None\n",
    "        self.daily_returns = []\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def add_technical_indicators(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Add technical indicators to the dataframe\n",
    "        \"\"\"\n",
    "        for stock in df.columns.levels[0]:\n",
    "            # Bollinger Bands\n",
    "            df[(stock, 'BBUpper')], df[(stock, 'BBMiddle')], df[(stock, 'BBLower')] = talib.BBANDS(\n",
    "                df[(stock, 'close')], timeperiod=20, nbdevup=2, nbdevdn=2\n",
    "            )\n",
    "            # MACD\n",
    "            df[(stock, 'MACD')], df[(stock, 'MACDSignal')], df[(stock, 'MACDHist')] = talib.MACD(\n",
    "                df[(stock, 'close')], fastperiod=12, slowperiod=26, signalperiod=9\n",
    "            )\n",
    "            # RSI - Relative Strength Index\n",
    "            df[(stock, 'RSI')] = talib.RSI(df[(stock, 'close')], timeperiod=14)\n",
    "            # CCI - Commodity Channel Index\n",
    "            df[(stock, 'CCI')] = talib.CCI(df[(stock, 'high')], df[(stock, 'low')], df[(stock, 'close')], timeperiod=14)\n",
    "            # ADX - Average Directional Index\n",
    "            df[(stock, 'ADX')] = talib.ADX(df[(stock, 'high')], df[(stock, 'low')], df[(stock, 'close')], timeperiod=14)\n",
    "            # ATR - Average True Range\n",
    "            df[(stock, 'ATR')] = talib.ATR(df[(stock, 'high')], df[(stock, 'low')], df[(stock, 'close')], timeperiod=14)\n",
    "            \n",
    "            # Handle NaN values\n",
    "            df.fillna(0, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed, options=options)\n",
    "        if seed is not None:\n",
    "            self.action_space.seed(int(self.np_random.uniform(0, seed)))\n",
    "\n",
    "        # Reload processed data\n",
    "        self.prices, self.signal_features = self._process_data()\n",
    "        \n",
    "        # Reset episode state variables\n",
    "        self._start_tick = self.window_size\n",
    "        self._end_tick = len(self.prices) - 1\n",
    "        self._truncated = False\n",
    "        self._current_tick = self._start_tick\n",
    "        self._last_trade_tick = self._current_tick - 1\n",
    "        self._total_reward = 0.0\n",
    "        self._total_profit = 0.0\n",
    "        self._trade_count = 0\n",
    "        self.history = {}\n",
    "        \n",
    "        # Reset trading statistics\n",
    "        self.daily_returns = []\n",
    "        self.min_reward = float('inf')\n",
    "        self.max_reward = float('-inf')\n",
    "        self._balance = self.initial_balance\n",
    "        self._shares_held = {stock: 0 for stock in self.df.columns.levels[0]}\n",
    "        self._last_action = np.zeros(len(self.df.columns.levels[0]))\n",
    "        self._transaction_cost = 0\n",
    "        self._step_transaction_cost = 0\n",
    "        \n",
    "        # Initialize portfolio value\n",
    "        self._portfolio_value = self._balance\n",
    "        self._prev_portfolio_value = self._portfolio_value\n",
    "        self._peak_portfolio_value = self._portfolio_value\n",
    "\n",
    "        return self._get_observation(), self._get_info()\n",
    "    \n",
    "    def step(self, action):\n",
    "        self._truncated = False\n",
    "        self._current_tick += 1\n",
    "        self._last_action = action\n",
    "    \n",
    "        if self._current_tick == self._end_tick:\n",
    "            self._truncated = True\n",
    "\n",
    "        # Update portfolio based on action\n",
    "        self._update_profit(action)\n",
    "        \n",
    "        # Calculate reward\n",
    "        step_reward = self._calculate_reward()\n",
    "        self._total_reward += step_reward\n",
    "    \n",
    "        # Update trade tracking\n",
    "        self._last_trade_tick = self._current_tick\n",
    "        \n",
    "        # Get observation and info\n",
    "        observation = self._get_observation()\n",
    "        info = self._get_info()\n",
    "        \n",
    "        # Update history\n",
    "        self._update_history(info)\n",
    "    \n",
    "        return observation, step_reward, False, self._truncated, info\n",
    "    \n",
    "    def _get_info(self):\n",
    "        \"\"\"\n",
    "        Return detailed information about current state\n",
    "        \"\"\"\n",
    "        detailed_info = {\n",
    "            'balance': self._balance,\n",
    "            'shares_held': self._shares_held,\n",
    "            'last_action': self._last_action,\n",
    "            'portfolio_value': self._portfolio_value,\n",
    "            'total_reward': self._total_reward,\n",
    "            'total_profit': self._total_profit,\n",
    "            'overall_transaction_cost': self._transaction_cost,\n",
    "            'average_daily_return': np.mean(self.daily_returns) if self.daily_returns else 0,\n",
    "        }\n",
    "        return detailed_info\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        Return observation window of features\n",
    "        \"\"\"\n",
    "        return self.signal_features[(self._current_tick - self.window_size+1):self._current_tick + 1]\n",
    "    \n",
    "    def _update_history(self, info):\n",
    "        \"\"\"\n",
    "        Update history dictionary with info from current step\n",
    "        \"\"\"\n",
    "        if not self.history:\n",
    "            self.history = {key: [] for key in info.keys()}\n",
    "        for key, value in info.items():\n",
    "            self.history[key].append(value)\n",
    "\n",
    "    def _process_data(self):\n",
    "        \"\"\"\n",
    "        Process raw data into normalized features\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _calculate_reward(self):\n",
    "        \"\"\"\n",
    "        Calculate reward for current step\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _update_profit(self, action):\n",
    "        \"\"\"\n",
    "        Update portfolio based on action\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VnStockEnv(TradingEnv):\n",
    "    def _process_data(self):\n",
    "        \"\"\"\n",
    "        Process multiindex DataFrame into prices and features\n",
    "        \"\"\"\n",
    "        feature_columns = [\n",
    "            'close', 'RSI', 'MACD', 'MACDSignal', 'MACDHist',\n",
    "            'BBUpper', 'BBLower', 'BBMiddle', 'CCI', 'ADX', 'ATR'\n",
    "        ]\n",
    "        prices = {}\n",
    "        signal_features = pd.DataFrame()\n",
    "        \n",
    "        for stock in self.df.columns.levels[0]:\n",
    "            # Get close prices for this stock\n",
    "            prices_close = self.df[(stock, 'close')].to_numpy()\n",
    "            # Extract price range based on frame_bound\n",
    "            prices_range = prices_close[self.frame_bound[0] - self.window_size : self.frame_bound[1]]\n",
    "            prices[stock] = prices_range\n",
    "            \n",
    "            # Process each feature\n",
    "            for feature in feature_columns:\n",
    "                if feature in self.df.columns.levels[1]:\n",
    "                    # Extract feature data\n",
    "                    data = self.df[(stock, feature)].iloc[self.frame_bound[0] - self.window_size:self.frame_bound[1]].values\n",
    "                    \n",
    "                    # Handle NaN values\n",
    "                    data = np.nan_to_num(data)\n",
    "                    \n",
    "                    # Normalize the feature\n",
    "                    if np.max(data) - np.min(data) != 0:\n",
    "                        normalized = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "                    else:\n",
    "                        normalized = data\n",
    "                    \n",
    "                    signal_features[(stock, feature)] = normalized\n",
    "        \n",
    "        return prices, signal_features.values\n",
    "\n",
    "    def _calculate_reward(self) -> float:\n",
    "        \"\"\"\n",
    "        Enhanced reward calculation that balances profit, risk-adjusted returns, \n",
    "        drawdown protection, and efficient trading\n",
    "        \"\"\"\n",
    "        portfolio_value = self._portfolio_value \n",
    "\n",
    "        # Basic reward: portfolio value change\n",
    "        base_reward = portfolio_value - self._prev_portfolio_value\n",
    "        \n",
    "        # Calculate daily return for metrics\n",
    "        daily_return = (portfolio_value - self._prev_portfolio_value) / self._prev_portfolio_value if self._prev_portfolio_value > 0 else 0\n",
    "        self.daily_returns.append(daily_return)\n",
    "        \n",
    "        # Calculate Sharpe ratio component (if we have enough data points)\n",
    "        sharpe_component = 0\n",
    "        if len(self.daily_returns) > 5:\n",
    "            returns_std = np.std(self.daily_returns[-20:]) if len(self.daily_returns) >= 20 else np.std(self.daily_returns)\n",
    "            avg_return = np.mean(self.daily_returns[-20:]) if len(self.daily_returns) >= 20 else np.mean(self.daily_returns)\n",
    "            \n",
    "            # Prevent division by zero\n",
    "            if returns_std > 0:\n",
    "                sharpe_ratio = (avg_return - self.risk_free_rate) / returns_std\n",
    "                sharpe_component = sharpe_ratio * 0.5  # Weight for Sharpe ratio\n",
    "        \n",
    "        # Advanced drawdown penalty with progressive scaling\n",
    "        drawdown_penalty = 0\n",
    "        if portfolio_value < self._peak_portfolio_value:\n",
    "            drawdown_percentage = (self._peak_portfolio_value - portfolio_value) / self._peak_portfolio_value\n",
    "            # Progressive penalty that grows exponentially with larger drawdowns\n",
    "            drawdown_penalty = 0.1 * (drawdown_percentage ** 2) * self._peak_portfolio_value\n",
    "        \n",
    "        # Transaction efficiency incentive\n",
    "        trade_efficiency = 0\n",
    "        if self._step_transaction_cost > 0:\n",
    "            profit_ratio = max(0, base_reward) / (self._step_transaction_cost + 1e-9)  # Prevent division by zero\n",
    "            if profit_ratio > 2.0:  # If profit is more than twice the transaction cost\n",
    "                trade_efficiency = 0.2 * profit_ratio  # Reward efficient trades\n",
    "            else:\n",
    "                trade_efficiency = -0.1 * self._step_transaction_cost  # Small penalty for inefficient trades\n",
    "        \n",
    "        # Holding time incentive to discourage excessive trading\n",
    "        holding_time_factor = 0\n",
    "        if self._current_tick - self._last_trade_tick > 3:  # If held for more than 3 time steps\n",
    "            holding_time_factor = 0.05 * base_reward if base_reward > 0 else 0\n",
    "        \n",
    "        # Portfolio diversification incentive\n",
    "        diversification_score = 0\n",
    "        non_zero_positions = sum(1 for shares in self._shares_held.values() if shares > 0)\n",
    "        if non_zero_positions > 0:\n",
    "            total_shares = sum(self._shares_held.values())\n",
    "            if total_shares > 0:\n",
    "                # Calculate Herfindahl index (measure of concentration)\n",
    "                herfindahl = sum((shares/total_shares)**2 for shares in self._shares_held.values() if shares > 0)\n",
    "                # Convert to diversification score (1 - herfindahl)\n",
    "                diversification_score = (1 - herfindahl) * 0.2 * base_reward if base_reward > 0 else 0\n",
    "        \n",
    "        # Momentum following bonus\n",
    "        momentum_bonus = 0\n",
    "        if len(self.daily_returns) >= 3:\n",
    "            recent_trend = np.mean(self.daily_returns[-3:])\n",
    "            if (recent_trend > 0 and base_reward > 0) or (recent_trend < 0 and base_reward < 0):\n",
    "                momentum_bonus = 0.15 * abs(base_reward)  # Reward for following the trend\n",
    "        \n",
    "        # Combine all components\n",
    "        reward = base_reward - self._step_transaction_cost + sharpe_component - drawdown_penalty + trade_efficiency + holding_time_factor + diversification_score + momentum_bonus\n",
    "        \n",
    "        # Update tracking variables\n",
    "        self._prev_portfolio_value = portfolio_value\n",
    "        self._peak_portfolio_value = max(self._peak_portfolio_value, portfolio_value)\n",
    "        \n",
    "        # Track min and max rewards for normalization\n",
    "        self.min_reward = min(self.min_reward, reward)\n",
    "        self.max_reward = max(self.max_reward, reward)\n",
    "        \n",
    "        # Normalize reward while preventing division by zero\n",
    "        if self.max_reward > self.min_reward:\n",
    "            normalized_reward = (reward - self.min_reward) / (self.max_reward - self.min_reward)\n",
    "        else:\n",
    "            normalized_reward = 0\n",
    "        \n",
    "        return normalized_reward\n",
    "    \n",
    "    def _update_profit(self, action: float):\n",
    "        \"\"\"\n",
    "        Update portfolio based on action\n",
    "        \"\"\"\n",
    "        self._step_transaction_cost = 0\n",
    "        \n",
    "        for i, stock in enumerate(self.df.columns.levels[0]):\n",
    "            current_price = self.prices[stock][self._current_tick]\n",
    "            shares_to_trade = int(action[i] * self.trade_max)  # Scale action to trade_max\n",
    "            \n",
    "            # Only trade if price is valid\n",
    "            if self.df[(stock, 'close')].iloc[self._current_tick] != 0:\n",
    "                # Handle buying shares\n",
    "                if shares_to_trade > 0:\n",
    "                    # Calculate maximum affordable shares considering balance, max_shares limit, and shares already held\n",
    "                    max_affordable = min(\n",
    "                        shares_to_trade,\n",
    "                        int(self._balance / current_price),\n",
    "                        self.max_shares - self._shares_held[stock]\n",
    "                    )\n",
    "        \n",
    "                    if max_affordable > 0:\n",
    "                        cost = max_affordable * current_price\n",
    "                        transaction_fee = cost * self.trade_fee_percent\n",
    "                        self._balance -= (cost + transaction_fee)\n",
    "                        self._shares_held[stock] += max_affordable\n",
    "                        self._step_transaction_cost += transaction_fee\n",
    "                        self._transaction_cost += transaction_fee\n",
    "                        self._trade_count += 1\n",
    "\n",
    "                # Handle selling shares\n",
    "                elif shares_to_trade < 0:\n",
    "                    shares_to_sell = min(abs(shares_to_trade), self._shares_held[stock])\n",
    "                    if shares_to_sell > 0:\n",
    "                        proceeds = shares_to_sell * current_price\n",
    "                        transaction_fee = proceeds * self.trade_fee_percent\n",
    "                        self._balance += (proceeds - transaction_fee)\n",
    "                        self._shares_held[stock] -= shares_to_sell\n",
    "                        self._step_transaction_cost += transaction_fee\n",
    "                        self._transaction_cost += transaction_fee\n",
    "                        self._trade_count += 1\n",
    "        \n",
    "        # Update portfolio value for all stocks\n",
    "        self._portfolio_value = self._balance + sum(\n",
    "            self._shares_held[stock] * self.prices[stock][self._current_tick]\n",
    "            for stock in self.df.columns.levels[0]\n",
    "        )\n",
    "        # Update peak portfolio value\n",
    "        self._peak_portfolio_value = max(self._peak_portfolio_value, self._portfolio_value)\n",
    "        \n",
    "        # Calculate profit/loss\n",
    "        self._total_profit = self._portfolio_value - self.initial_balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter for multiple algorithms MUST ADJUST\n",
    "WINDOW_SIZE = [5, 10, 25]\n",
    "\n",
    "HYPERPARAMS_PPO = [\n",
    "    {\n",
    "        \"name\": \"PPO_default\",\n",
    "        \"policy\": \"MlpPolicy\",\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"n_steps\": 2048,\n",
    "        \"batch_size\": 64,\n",
    "        \"n_epochs\": 10,\n",
    "        \"gamma\": 0.99,\n",
    "        \"clip_range\": 0.2,\n",
    "        \"ent_coef\": 0.0,\n",
    "        \"vf_coef\": 0.5,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"window_size\": 25,\n",
    "    }\n",
    "]\n",
    "\n",
    "HYPERPARAMS_A2C = [\n",
    "    {\n",
    "        \"name\": \"A2C_default\",\n",
    "        \"policy\": \"MlpPolicy\",\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"n_steps\": 5,\n",
    "        \"gamma\": 0.99,\n",
    "        \"ent_coef\": 0.0,\n",
    "        \"vf_coef\": 0.5,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "    }\n",
    "]\n",
    "\n",
    "HYPERPARAMS_SAC = [\n",
    "    {\n",
    "        \"name\": \"SAC_default\",\n",
    "        \"policy\": \"MlpPolicy\",\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"buffer_size\": 1000000,\n",
    "        \"batch_size\": 256,\n",
    "        \"gamma\": 0.99,\n",
    "        \"tau\": 0.005,\n",
    "        \"train_freq\": 1,\n",
    "        \"gradient_steps\": 1,\n",
    "        \"ent_coef\": \"auto\",\n",
    "    }\n",
    "]\n",
    "\n",
    "HYPERPARAMS_RPPO = [\n",
    "    {\n",
    "        \"name\": \"RPPO_default\",\n",
    "        \"policy\": \"MlpLstmPolicy\",\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"n_steps\": 2048,\n",
    "        \"batch_size\": 64,\n",
    "        \"n_epochs\": 10,\n",
    "        \"gamma\": 0.99,\n",
    "        \"clip_range\": 0.2,\n",
    "        \"ent_coef\": 0.0,\n",
    "        \"vf_coef\": 0.5,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment(data, window_size, initial_balance, max_shares, trade_max):\n",
    "    \"\"\"Create and configure trading environment\"\"\"\n",
    "    # Set up environment parameters\n",
    "    frame_bound = (window_size, len(data))\n",
    "\n",
    "    # Create environment\n",
    "    env = VnStockEnv(\n",
    "        df=data,\n",
    "        window_size=window_size,\n",
    "        frame_bound=frame_bound,\n",
    "        initial_balance=initial_balance,\n",
    "        max_shares=max_shares,\n",
    "        trade_max=trade_max\n",
    "    )\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    print(f\"Observation shape: {observation.shape}\")\n",
    "        \n",
    "    return env\n",
    "\n",
    "def train_model(model_class, params, data, initial_balance, max_shares, trade_max, timesteps=500000):\n",
    "    \"\"\"Train a model with specific parameters\"\"\"\n",
    "    # Create environment\n",
    "    window_size = params.pop(\"window_size\")\n",
    "    env = create_environment(data, window_size, initial_balance, max_shares, trade_max)\n",
    "    \n",
    "    # Setup monitoring\n",
    "    model_name = params.pop(\"name\")\n",
    "    os.makedirs(f\"models/{model_name}\", exist_ok=True)\n",
    "    os.makedirs(f\"logs/{model_name}\", exist_ok=True)\n",
    "    \n",
    "    env = Monitor(env, f\"logs/{model_name}\")\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    \n",
    "    # Create callback\n",
    "    callback = CustomLoggingCallback(\n",
    "        verbose = 1, \n",
    "        log_interval = 20000, \n",
    "        save_path=f\"models/{model_name}\", \n",
    "        filename=f\"{model_name}_training.pdf\"\n",
    "    )\n",
    "    \n",
    "    # Create and train model\n",
    "    model = model_class(**params, env=env, verbose=0, device=device)\n",
    "    model.learn(total_timesteps=timesteps, callback=callback)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = f\"models/{model_name}/{model_name}\"\n",
    "    model.save(model_path)\n",
    "    \n",
    "    # Reset params for next use\n",
    "    params[\"window_size\"] = window_size\n",
    "    params[\"name\"] = model_name\n",
    "    \n",
    "    return model_path, window_size\n",
    "\n",
    "def evaluate_model(model_path, model_class, data, initial_balance, max_shares, trade_max, window_size):\n",
    "    \"\"\"Evaluate a trained model on test data\"\"\"\n",
    "    # Create test environment\n",
    "    env = create_environment(data, window_size, initial_balance, max_shares, trade_max)\n",
    "    env = Monitor(env, None)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    \n",
    "    # Load model\n",
    "    model = model_class.load(model_path)\n",
    "    \n",
    "    # Run evaluation episodes\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    truncated = False\n",
    "    \n",
    "    # Track performance metrics\n",
    "    portfolio_values = []\n",
    "    returns = []\n",
    "    actions = []\n",
    "    dates = data.index[window_size:]\n",
    "    \n",
    "    while not (done or truncated):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        portfolio_values.append(info[0]['portfolio_value'])\n",
    "        \n",
    "        # Calculate daily return\n",
    "        if len(portfolio_values) > 1:\n",
    "            daily_return = (portfolio_values[-1] - portfolio_values[-2]) / portfolio_values[-2]\n",
    "            returns.append(daily_return)\n",
    "        \n",
    "        actions.append(action)\n",
    "    \n",
    "    return portfolio_values, returns, actions, dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate / Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stocks using: 31\n",
      "Loading TRAIN data...\n",
      "Fetching 14 Vietnamese stock data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 17 Global stock data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TEST data...\n",
      "Fetching 14 Vietnamese stock data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 17 Global stock data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "\n",
      "Training PPO models...\n",
      "  Training PPO_default...\n",
      "Observation shape: (25, 341)\n",
      "Step: 20000, Avg Reward: 0.5068773627281189, Max Reward: 1.0, Min Reward: 0.0\n",
      "Step: 40000, Avg Reward: 0.6651334762573242, Max Reward: 1.0, Min Reward: 0.0\n",
      "Step: 60000, Avg Reward: 0.7068259119987488, Max Reward: 1.0, Min Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    STOCK_LIST_VN = ['FPT', 'VCB', 'VIC', 'GAS', 'VHM', 'BID', 'MBB', 'HVN', 'HPG', 'GVR', 'VNM', 'SSI', 'GEX', 'ACB']\n",
    "    STOCK_LIST_GLOBAL = ['AXP', 'AAPL', 'BA', 'CAT', 'CSCO', 'CVX', 'GS', 'HON', 'IBM', 'INTC', 'JPM', 'MSFT', 'NKE', 'GOOGL', 'AMZN', 'WMT', 'NVDA']\n",
    "    print(f\"Number of stocks using: {len(STOCK_LIST_VN) + len(STOCK_LIST_GLOBAL)}\")\n",
    "    \n",
    "    print(\"Loading TRAIN data...\")\n",
    "    START_DATE = '2018-01-01'\n",
    "    END_DATE = '2024-01-01'\n",
    "    train_data = fetch_stock_data(STOCK_LIST_VN, STOCK_LIST_GLOBAL, START_DATE, END_DATE)\n",
    "    \n",
    "    train_data.head()\n",
    "    \n",
    "    print(\"Loading TEST data...\")\n",
    "    START_DATE = '2024-01-01'\n",
    "    END_DATE = '2025-04-13'\n",
    "    test_data = fetch_stock_data(STOCK_LIST_VN, STOCK_LIST_GLOBAL, START_DATE, END_DATE)\n",
    "    \n",
    "    test_data.head()\n",
    "    print(\"Data loaded successfully.\")\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "    os.makedirs(\"plots\", exist_ok=True)\n",
    "    \n",
    "    # Dictionary to store results for all models\n",
    "    all_results = {}\n",
    "    \n",
    "    # Define model classes and their hyperparameter lists\n",
    "    models_to_train = [\n",
    "        (PPO, HYPERPARAMS_PPO),\n",
    "        # (A2C, HYPERPARAMS_A2C),\n",
    "        # (SAC, HYPERPARAMS_SAC),\n",
    "        # (RecurrentPPO, HYPERPARAMS_RPPO)\n",
    "    ]\n",
    "    \n",
    "    # Train all models with different hyperparameters\n",
    "    for model_class, hyperparams_list in models_to_train:\n",
    "        model_type = model_class.__name__\n",
    "        print(f\"\\nTraining {model_type} models...\")\n",
    "        \n",
    "        for params in hyperparams_list:\n",
    "            model_name = params[\"name\"]\n",
    "            print(f\"  Training {model_name}...\")\n",
    "            \n",
    "            # Create a copy of params to avoid modifying the original\n",
    "            params_copy = params.copy()\n",
    "            \n",
    "            # Train model\n",
    "            model_path, window_size = train_model(\n",
    "                model_class=model_class,\n",
    "                params=params_copy,\n",
    "                data=train_data,\n",
    "                initial_balance=100000,\n",
    "                max_shares=1000,\n",
    "                trade_max=100,\n",
    "                timesteps=500000\n",
    "            )\n",
    "            \n",
    "            # Evaluate model\n",
    "            portfolio_values, returns, actions, dates = evaluate_model(\n",
    "                model_path=model_path,\n",
    "                model_class=model_class,\n",
    "                data=test_data,\n",
    "                initial_balance=100000,\n",
    "                max_shares=1000,\n",
    "                trade_max=100,\n",
    "                window_size=window_size\n",
    "            )\n",
    "            \n",
    "            # Visualize results\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.plot(dates, portfolio_values, label=f\"{model_name} Portfolio Value\")\n",
    "            plt.title(f\"{model_name} Portfolio Value Over Time\")\n",
    "            plt.xlabel(\"Date\")\n",
    "            plt.ylabel(\"Portfolio Value\")\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "            plt.tight_layout()\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
